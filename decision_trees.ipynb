{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Выберем необходимые нам строки из прошлого дз для реализации регресии","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(load_boston()['filename'], skiprows=1)","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  dtype=np.int):\n","output_type":"stream"}]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n\n        B  LSTAT  MEDV  \n0  396.90   4.98  24.0  \n1  396.90   9.14  21.6  \n2  392.83   4.03  34.7  \n3  394.63   2.94  33.4  \n4  396.90   5.33  36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, positive=False):\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n/srv/conda/envs/notebook/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_score(X,y, random_seed=42, model=None):\n    '''Функция возвращает % качества работы модели'''\n    if model is None:\n        model = LinearRegression()\n        X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=random_seed)\n        model.fit(X_train, y_train)\n        return model.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"columns = ['CHAS',  'RM', 'RAD', 'TAX', 'LSTAT', 'PTRATIO', 'NOX',  'DIS',  'ZN']","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### наш результат регресии:","metadata":{}},{"cell_type":"code","source":"get_score(data[columns], data['MEDV'])","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0.7183142507178601"},"metadata":{}}]},{"cell_type":"code","source":"data_reg = data.copy()\ndata_reg.head()","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n\n        B  LSTAT  MEDV  \n0  396.90   4.98  24.0  \n1  396.90   9.14  21.6  \n2  392.83   4.03  34.7  \n3  394.63   2.94  33.4  \n4  396.90   5.33  36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1</td>\n      <td>296</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2</td>\n      <td>242</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3</td>\n      <td>222</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_reg['taxrm'] = data_reg['TAX'] * data_reg['RM']\ndata_reg['taxdis'] = data_reg['TAX'] * data_reg['LSTAT']\ncolumns_reg = ['CHAS',  'RM', 'RAD', 'TAX', 'LSTAT', 'PTRATIO', 'NOX',  'DIS',  'ZN', 'taxdis', 'taxrm']","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### наш улучшенный результат регресии:","metadata":{}},{"cell_type":"code","source":"get_score(data_reg[columns_reg], data_reg['MEDV'])","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0.7844089601546259"},"metadata":{}}]},{"cell_type":"markdown","source":"Деревье решений","metadata":{}},{"cell_type":"code","source":"# импортирем функцию для кол-ва вхождений, чтобы использовать наши функции\nfrom collections import Counter","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"$H(R) = -\\sum_{k=1}^{K}p_klogp_k$","metadata":{}},{"cell_type":"code","source":"def HEntropy(x):\n    \"\"\"функция для расчёта энтропийного критерия качества\"\"\"\n    length = len(x)\n    cnt = Counter(x)\n    \n    ent = 0\n    for cl in cnt.values():\n        p = cl / length\n        l2 = np.log2(p)\n        it = -p * l2\n        ent += it\n    \n    return ent","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"$Н(R) = -\\sum_{k=1}^{K}p_k(1-p_k)$","metadata":{}},{"cell_type":"code","source":"def HEntropy(l):\n    \"\"\"функция расчёта энтропийного критерия множества\"\"\"\n    length = len(l)\n    cnt = Counter(l)\n    \n    ent = 0\n    for cl in cnt.values():\n        p = cl / length\n        l2 = np.log2(p)\n        it = -p * l2\n        ent += it\n    \n    return ent","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"$Н(R) = -\\sum_{k=1}^{K}p_k(1-p_k)$","metadata":{}},{"cell_type":"code","source":"def HGini(l):\n    \"\"\"Функция для рассчета критерия Джини\"\"\"\n    length = len(l)\n    cnt = Counter(l)\n    \n    gini = 0\n    for cl in cnt.values():\n        p_1 = cl / length\n        p_2 = (1 - p_1)\n        it = p_1 * p_2\n        gini += it\n    \n    return gini","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"$IG(R) = H(R) - \\frac{|R_l|}{|R|}H(R_l) - \\frac{|R_r|}{|R|}H(R_r)$","metadata":{}},{"cell_type":"code","source":"def IG(H, l, i):\n    \"\"\"Information Gain (IG)* - функционал качества, отвечающий на вопрос, а сколько энтропии мы погасили при определённом разбиении? \n    На каждом шаге разбиения при построении дерева максимизируется IG\"\"\"\n    left_l = l[:i]\n    right_l = l[i:]\n    return H(l) - (len(left_l) / len(l)) * H(left_l) - (len(right_l) / len(l)) * H(right_l)\n","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def test_H(H, l):\n    \"\"\"функция для визуализации работы произвольного критерия качества на выборке\"\"\"\n    print(\"{:5} {:3}   {:4} {:4} {:4}\".format(\"#\",\"l\",\"IG\",\"Hl\",\"Hr\"))\n    print(\"-\"*24)\n    i_max,IG_max=0,0\n    for i in range(0,len(l)):\n        print(f\"{i:2}. {l[i]:3}   {IG(H, l, i):.2f} {H(l[:i]):.2f} {H(l[i:]):.2f}\")\n        if IG_max < IG(H, l, i):\n            i_max, IG_max = i, IG(H, l, i)\n    print(f'Деление после элемента:{i_max-1}')","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# запишем нашу целевую переменную в массив\nmedv = np.array(data.MEDV)\n# отсортируем переменную\nsort_medv = sorted(medv)","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"HEntropy(medv)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"7.527082109018076"},"metadata":{}}]},{"cell_type":"code","source":"# посмотрим энтропию\ntest_H(HEntropy, medv)","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"#     l     IG   Hl   Hr  \n------------------------\n 0. 24.0   0.00 0.00 7.53\n 1. 21.6   0.02 0.00 7.53\n 2. 34.7   0.03 1.00 7.52\n 3. 33.4   0.04 1.58 7.52\n 4. 36.2   0.05 2.00 7.52\n 5. 28.7   0.06 2.32 7.51\n 6. 22.9   0.07 2.58 7.51\n 7. 27.1   0.08 2.81 7.51\n 8. 16.5   0.09 3.00 7.51\n 9. 18.9   0.09 3.17 7.51\n10. 15.0   0.10 3.32 7.51\n11. 18.9   0.10 3.46 7.51\n12. 21.7   0.11 3.42 7.51\n13. 20.4   0.12 3.55 7.51\n14. 18.2   0.12 3.66 7.51\n15. 19.9   0.12 3.77 7.51\n16. 23.1   0.13 3.88 7.52\n17. 17.5   0.13 3.97 7.52\n18. 20.2   0.13 4.06 7.52\n19. 18.2   0.14 4.14 7.52\n20. 13.6   0.15 4.12 7.51\n21. 19.6   0.15 4.20 7.51\n22. 15.2   0.15 4.28 7.51\n23. 14.5   0.16 4.35 7.51\n24. 15.6   0.16 4.42 7.51\n25. 13.9   0.16 4.48 7.52\n26. 16.6   0.17 4.55 7.51\n27. 14.8   0.17 4.61 7.51\n28. 18.4   0.18 4.66 7.51\n29. 21.0   0.18 4.72 7.51\n30. 12.7   0.18 4.77 7.51\n31. 14.5   0.19 4.83 7.51\n32. 13.2   0.19 4.81 7.50\n33. 13.1   0.20 4.86 7.50\n34. 13.5   0.20 4.91 7.50\n35. 18.9   0.21 4.96 7.50\n36. 20.0   0.22 4.93 7.49\n37. 21.0   0.22 4.97 7.50\n38. 24.7   0.22 4.96 7.49\n39. 30.8   0.22 5.01 7.49\n40. 34.9   0.23 5.05 7.49\n41. 26.6   0.23 5.10 7.49\n42. 25.3   0.23 5.14 7.49\n43. 24.7   0.24 5.18 7.48\n44. 21.2   0.25 5.17 7.48\n45. 19.3   0.25 5.21 7.48\n46. 20.0   0.25 5.25 7.48\n47. 16.6   0.25 5.24 7.48\n48. 14.4   0.26 5.24 7.48\n49. 19.4   0.26 5.27 7.48\n50. 19.7   0.26 5.31 7.48\n51. 20.5   0.26 5.34 7.48\n52. 25.0   0.27 5.38 7.48\n53. 23.4   0.26 5.41 7.48\n54. 18.9   0.26 5.44 7.48\n55. 35.4   0.28 5.42 7.47\n56. 24.7   0.28 5.45 7.47\n57. 31.6   0.29 5.43 7.47\n58. 23.3   0.29 5.47 7.46\n59. 19.6   0.29 5.50 7.46\n60. 18.7   0.30 5.49 7.47\n61. 16.0   0.30 5.52 7.47\n62. 22.2   0.30 5.55 7.46\n63. 25.0   0.30 5.58 7.46\n64. 33.0   0.30 5.58 7.46\n65. 23.5   0.31 5.61 7.46\n66. 19.4   0.31 5.64 7.45\n67. 22.0   0.31 5.64 7.45\n68. 17.4   0.31 5.66 7.46\n69. 20.9   0.31 5.69 7.46\n70. 24.2   0.31 5.72 7.45\n71. 21.7   0.32 5.74 7.45\n72. 22.8   0.32 5.74 7.45\n73. 23.4   0.32 5.77 7.45\n74. 24.1   0.33 5.77 7.45\n75. 21.4   0.33 5.79 7.45\n76. 20.0   0.32 5.82 7.45\n77. 20.8   0.33 5.81 7.45\n78. 21.2   0.33 5.83 7.45\n79. 20.3   0.33 5.83 7.45\n80. 28.0   0.33 5.85 7.45\n81. 23.9   0.33 5.88 7.44\n82. 24.8   0.33 5.90 7.45\n83. 22.9   0.33 5.92 7.45\n84. 23.9   0.33 5.92 7.45\n85. 26.6   0.33 5.92 7.45\n86. 22.5   0.34 5.92 7.45\n87. 22.2   0.34 5.94 7.45\n88. 23.6   0.34 5.94 7.45\n89. 28.7   0.34 5.96 7.45\n90. 22.6   0.35 5.96 7.45\n91. 22.0   0.34 5.99 7.45\n92. 22.9   0.34 5.99 7.45\n93. 25.0   0.35 5.98 7.45\n94. 20.6   0.35 5.97 7.45\n95. 28.4   0.35 5.99 7.45\n96. 21.4   0.35 6.01 7.45\n97. 38.7   0.35 6.01 7.45\n98. 43.8   0.35 6.03 7.45\n99. 33.2   0.36 6.05 7.44\n100. 27.5   0.36 6.07 7.44\n101. 26.5   0.35 6.09 7.44\n102. 18.6   0.36 6.11 7.43\n103. 19.3   0.36 6.13 7.43\n104. 20.1   0.36 6.13 7.43\n105. 19.5   0.36 6.15 7.44\n106. 19.5   0.35 6.17 7.44\n107. 20.4   0.36 6.17 7.44\n108. 19.8   0.36 6.17 7.44\n109. 19.4   0.36 6.19 7.44\n110. 21.7   0.36 6.18 7.44\n111. 22.8   0.36 6.18 7.44\n112. 18.8   0.36 6.18 7.44\n113. 18.7   0.36 6.20 7.44\n114. 18.5   0.37 6.20 7.44\n115. 18.3   0.36 6.21 7.44\n116. 21.2   0.36 6.23 7.44\n117. 19.2   0.37 6.23 7.44\n118. 20.4   0.37 6.24 7.44\n119. 19.3   0.37 6.24 7.44\n120. 22.0   0.37 6.23 7.44\n121. 20.3   0.38 6.23 7.44\n122. 20.5   0.38 6.23 7.44\n123. 17.3   0.38 6.23 7.44\n124. 18.8   0.38 6.25 7.43\n125. 21.4   0.39 6.25 7.43\n126. 15.7   0.40 6.24 7.43\n127. 16.2   0.40 6.26 7.42\n128. 18.0   0.40 6.28 7.42\n129. 14.3   0.40 6.30 7.41\n130. 19.2   0.40 6.31 7.41\n131. 19.6   0.41 6.31 7.40\n132. 23.0   0.41 6.31 7.40\n133. 18.4   0.41 6.33 7.40\n134. 15.6   0.41 6.33 7.40\n135. 18.1   0.41 6.33 7.41\n136. 17.4   0.41 6.34 7.40\n137. 17.1   0.42 6.35 7.40\n138. 13.3   0.41 6.36 7.40\n139. 17.8   0.41 6.38 7.40\n140. 14.0   0.41 6.39 7.40\n141. 14.4   0.41 6.41 7.39\n142. 13.4   0.41 6.41 7.39\n143. 15.6   0.41 6.43 7.39\n144. 11.8   0.41 6.42 7.39\n145. 13.8   0.41 6.44 7.39\n146. 15.6   0.41 6.45 7.39\n147. 14.6   0.41 6.44 7.39\n148. 17.8   0.41 6.46 7.39\n149. 15.4   0.41 6.46 7.39\n150. 21.5   0.41 6.48 7.39\n151. 19.6   0.41 6.49 7.39\n152. 15.3   0.41 6.48 7.38\n153. 19.4   0.42 6.50 7.38\n154. 17.0   0.42 6.49 7.38\n155. 15.6   0.42 6.50 7.37\n156. 13.1   0.43 6.50 7.36\n157. 41.3   0.43 6.50 7.36\n158. 24.3   0.43 6.51 7.36\n159. 23.3   0.43 6.53 7.36\n160. 27.0   0.43 6.53 7.36\n161. 50.0   0.43 6.54 7.35\n162. 50.0   0.43 6.55 7.36\n163. 50.0   0.42 6.56 7.37\n164. 22.7   0.42 6.55 7.38\n165. 25.0   0.42 6.57 7.37\n166. 50.0   0.42 6.56 7.38\n167. 23.8   0.42 6.55 7.39\n168. 23.8   0.41 6.57 7.39\n169. 22.3   0.41 6.57 7.39\n170. 17.4   0.41 6.58 7.39\n171. 19.1   0.42 6.58 7.38\n172. 23.1   0.41 6.59 7.38\n173. 23.6   0.41 6.60 7.39\n174. 22.6   0.42 6.60 7.38\n175. 29.4   0.42 6.60 7.38\n176. 23.2   0.42 6.61 7.37\n177. 24.6   0.41 6.62 7.38\n178. 29.9   0.41 6.64 7.38\n179. 37.2   0.41 6.65 7.37\n180. 39.8   0.41 6.66 7.36\n181. 36.2   0.42 6.68 7.35\n182. 37.9   0.42 6.68 7.35\n183. 32.5   0.42 6.69 7.34\n184. 26.4   0.43 6.70 7.33\n185. 29.6   0.42 6.71 7.33\n186. 50.0   0.42 6.73 7.33\n187. 32.0   0.42 6.72 7.34\n188. 29.8   0.42 6.73 7.33\n189. 34.9   0.41 6.74 7.33\n190. 37.0   0.42 6.74 7.33\n191. 30.5   0.42 6.76 7.32\n192. 36.4   0.42 6.77 7.32\n193. 31.1   0.42 6.78 7.31\n194. 29.1   0.42 6.79 7.30\n195. 50.0   0.42 6.80 7.30\n196. 33.3   0.42 6.79 7.31\n197. 30.3   0.42 6.81 7.30\n198. 34.6   0.42 6.82 7.29\n199. 34.9   0.42 6.83 7.28\n200. 32.9   0.43 6.83 7.28\n201. 24.1   0.43 6.84 7.27\n202. 42.3   0.43 6.84 7.27\n203. 48.5   0.43 6.85 7.26\n204. 50.0   0.43 6.86 7.25\n205. 22.6   0.43 6.85 7.26\n206. 24.4   0.43 6.85 7.26\n207. 22.5   0.43 6.86 7.26\n208. 24.4   0.43 6.86 7.26\n209. 20.0   0.43 6.86 7.26\n210. 21.7   0.43 6.86 7.26\n211. 19.3   0.43 6.85 7.26\n212. 22.4   0.44 6.85 7.26\n213. 28.1   0.43 6.86 7.26\n214. 23.7   0.44 6.87 7.25\n215. 25.0   0.43 6.88 7.26\n216. 23.3   0.43 6.88 7.26\n217. 28.7   0.43 6.87 7.26\n218. 21.5   0.44 6.87 7.25\n219. 23.0   0.44 6.87 7.24\n220. 26.7   0.44 6.88 7.24\n221. 21.7   0.44 6.89 7.23\n222. 27.5   0.45 6.88 7.24\n223. 30.1   0.45 6.88 7.24\n224. 44.8   0.44 6.89 7.24\n225. 50.0   0.44 6.90 7.23\n226. 37.6   0.44 6.89 7.24\n227. 31.6   0.44 6.91 7.23\n228. 46.7   0.45 6.91 7.22\n229. 31.5   0.45 6.92 7.21\n230. 24.3   0.44 6.93 7.21\n231. 31.7   0.45 6.93 7.21\n232. 41.7   0.45 6.94 7.20\n233. 48.3   0.45 6.95 7.19\n234. 29.0   0.45 6.96 7.18\n235. 24.0   0.44 6.97 7.18\n236. 25.1   0.45 6.97 7.17\n237. 31.5   0.45 6.98 7.17\n238. 23.7   0.45 6.98 7.16\n239. 23.3   0.45 6.98 7.16\n240. 22.0   0.46 6.98 7.15\n241. 20.1   0.46 6.98 7.15\n242. 22.2   0.46 6.98 7.16\n243. 23.7   0.46 6.98 7.16\n244. 17.6   0.46 6.98 7.16\n245. 18.5   0.46 6.99 7.15\n246. 24.3   0.46 6.99 7.15\n247. 20.5   0.46 6.99 7.14\n248. 24.5   0.47 6.98 7.13\n249. 26.2   0.46 6.99 7.13\n250. 24.4   0.46 7.00 7.12\n251. 24.8   0.46 7.00 7.12\n252. 29.6   0.46 7.00 7.13\n253. 42.8   0.47 7.01 7.12\n254. 21.9   0.47 7.01 7.11\n255. 20.9   0.46 7.02 7.11\n256. 44.0   0.46 7.03 7.10\n257. 50.0   0.46 7.04 7.09\n258. 36.0   0.46 7.03 7.10\n259. 30.1   0.46 7.04 7.09\n260. 33.8   0.46 7.04 7.09\n261. 43.1   0.46 7.05 7.08\n262. 48.8   0.46 7.06 7.07\n263. 31.0   0.46 7.07 7.06\n264. 36.5   0.46 7.07 7.05\n265. 22.8   0.46 7.08 7.04\n266. 30.7   0.46 7.08 7.04\n267. 50.0   0.46 7.09 7.03\n268. 43.5   0.47 7.08 7.04\n269. 20.7   0.46 7.09 7.03\n270. 21.1   0.46 7.10 7.03\n271. 25.2   0.46 7.11 7.03\n272. 24.4   0.46 7.12 7.02\n273. 35.2   0.46 7.12 7.01\n274. 32.4   0.46 7.12 7.00\n275. 32.0   0.46 7.13 6.99\n276. 33.2   0.46 7.14 6.98\n277. 33.1   0.47 7.14 6.97\n278. 29.1   0.46 7.15 6.96\n279. 35.1   0.47 7.15 6.95\n280. 45.4   0.47 7.16 6.94\n281. 35.4   0.47 7.16 6.93\n282. 46.0   0.47 7.17 6.92\n283. 50.0   0.47 7.17 6.91\n284. 32.2   0.47 7.17 6.92\n285. 22.0   0.47 7.17 6.91\n286. 20.1   0.47 7.17 6.91\n287. 23.2   0.47 7.17 6.91\n288. 22.3   0.47 7.17 6.92\n289. 24.8   0.47 7.17 6.90\n290. 28.5   0.47 7.17 6.90\n291. 37.3   0.47 7.18 6.89\n292. 27.9   0.47 7.19 6.88\n293. 23.9   0.46 7.20 6.88\n294. 21.7   0.46 7.20 6.88\n295. 28.6   0.47 7.19 6.88\n296. 27.1   0.46 7.20 6.87\n297. 20.3   0.47 7.20 6.86\n298. 22.5   0.47 7.20 6.86\n299. 29.0   0.47 7.20 6.85\n300. 24.8   0.48 7.20 6.84\n301. 22.0   0.48 7.20 6.83\n302. 26.4   0.48 7.19 6.82\n303. 33.1   0.49 7.19 6.81\n304. 36.1   0.49 7.20 6.80\n305. 28.4   0.49 7.20 6.79\n306. 33.4   0.49 7.21 6.78\n307. 28.2   0.49 7.21 6.77\n308. 22.8   0.49 7.22 6.75\n309. 20.3   0.50 7.21 6.74\n310. 16.1   0.50 7.21 6.73\n311. 22.1   0.50 7.22 6.73\n312. 19.4   0.49 7.23 6.72\n313. 21.6   0.50 7.22 6.72\n314. 23.8   0.50 7.22 6.71\n315. 16.2   0.50 7.22 6.71\n316. 17.8   0.50 7.23 6.69\n317. 19.8   0.50 7.22 6.70\n318. 23.1   0.50 7.23 6.69\n319. 21.0   0.49 7.23 6.70\n320. 23.8   0.50 7.23 6.69\n321. 23.1   0.50 7.22 6.68\n322. 20.4   0.50 7.22 6.68\n323. 18.5   0.51 7.22 6.67\n324. 25.0   0.51 7.22 6.67\n325. 24.6   0.51 7.21 6.67\n326. 23.0   0.51 7.22 6.66\n327. 22.2   0.51 7.22 6.66\n328. 19.3   0.51 7.21 6.66\n329. 22.6   0.52 7.21 6.64\n330. 19.8   0.52 7.21 6.64\n331. 17.1   0.52 7.21 6.63\n332. 19.4   0.52 7.21 6.63\n333. 22.2   0.52 7.21 6.61\n334. 20.7   0.53 7.20 6.60\n335. 21.1   0.53 7.20 6.59\n336. 19.5   0.53 7.21 6.57\n337. 18.5   0.53 7.21 6.57\n338. 20.6   0.54 7.20 6.56\n339. 19.0   0.53 7.21 6.57\n340. 18.7   0.53 7.21 6.57\n341. 32.7   0.53 7.21 6.55\n342. 16.5   0.53 7.22 6.54\n343. 23.9   0.53 7.22 6.52\n344. 31.2   0.53 7.22 6.52\n345. 17.5   0.53 7.23 6.51\n346. 17.2   0.53 7.23 6.51\n347. 23.1   0.52 7.24 6.51\n348. 24.5   0.52 7.24 6.51\n349. 26.6   0.51 7.24 6.51\n350. 22.9   0.52 7.24 6.50\n351. 24.1   0.52 7.24 6.48\n352. 18.6   0.53 7.24 6.47\n353. 30.1   0.53 7.24 6.45\n354. 18.2   0.53 7.24 6.44\n355. 20.6   0.53 7.24 6.42\n356. 17.8   0.53 7.24 6.43\n357. 21.7   0.53 7.24 6.43\n358. 22.7   0.53 7.23 6.41\n359. 22.6   0.54 7.23 6.40\n360. 25.0   0.54 7.23 6.38\n361. 19.9   0.54 7.23 6.38\n362. 20.8   0.54 7.23 6.39\n363. 16.8   0.54 7.23 6.38\n364. 21.9   0.53 7.24 6.38\n365. 27.5   0.53 7.24 6.38\n366. 21.9   0.53 7.24 6.38\n367. 23.1   0.53 7.24 6.36\n368. 50.0   0.53 7.24 6.36\n369. 50.0   0.53 7.23 6.37\n370. 50.0   0.53 7.22 6.38\n371. 50.0   0.53 7.22 6.39\n372. 50.0   0.54 7.21 6.38\n373. 13.8   0.54 7.20 6.37\n374. 13.8   0.54 7.21 6.38\n375. 15.0   0.53 7.21 6.38\n376. 13.9   0.53 7.21 6.38\n377. 13.3   0.53 7.21 6.37\n378. 13.1   0.53 7.21 6.36\n379. 10.2   0.53 7.21 6.36\n380. 10.4   0.52 7.22 6.37\n381. 10.9   0.51 7.23 6.37\n382. 11.3   0.50 7.23 6.37\n383. 12.3   0.50 7.24 6.35\n384. 8.8   0.50 7.25 6.34\n385. 7.2   0.49 7.26 6.34\n386. 10.5   0.48 7.26 6.34\n387. 7.4   0.48 7.27 6.34\n388. 10.2   0.47 7.28 6.33\n389. 11.5   0.47 7.28 6.33\n390. 15.1   0.46 7.29 6.31\n391. 23.2   0.46 7.29 6.29\n392. 9.7   0.46 7.29 6.29\n393. 13.8   0.46 7.30 6.27\n394. 12.7   0.45 7.30 6.27\n395. 13.1   0.45 7.30 6.27\n396. 12.5   0.45 7.30 6.26\n397. 8.5   0.45 7.31 6.24\n398. 5.0   0.44 7.31 6.24\n399. 6.3   0.43 7.32 6.24\n400. 5.6   0.43 7.33 6.22\n401. 7.2   0.43 7.33 6.20\n402. 12.1   0.42 7.34 6.20\n403. 8.3   0.42 7.34 6.18\n404. 8.5   0.41 7.35 6.18\n405. 5.0   0.41 7.35 6.17\n406. 11.9   0.41 7.35 6.15\n407. 27.9   0.40 7.36 6.15\n408. 17.2   0.40 7.36 6.13\n409. 27.5   0.40 7.36 6.13\n410. 15.0   0.40 7.36 6.11\n411. 17.2   0.40 7.36 6.09\n412. 17.9   0.40 7.36 6.07\n413. 16.3   0.40 7.37 6.05\n414. 7.0   0.40 7.38 6.03\n415. 7.2   0.39 7.38 6.03\n416. 7.5   0.39 7.38 6.01\n417. 10.4   0.38 7.39 5.98\n418. 8.8   0.38 7.39 5.96\n419. 8.4   0.38 7.39 5.94\n420. 16.7   0.37 7.40 5.94\n421. 14.2   0.37 7.41 5.94\n422. 20.8   0.36 7.41 5.92\n423. 13.4   0.36 7.41 5.90\n424. 11.7   0.36 7.42 5.91\n425. 8.3   0.35 7.42 5.91\n426. 10.2   0.35 7.42 5.88\n427. 10.9   0.35 7.42 5.86\n428. 11.0   0.35 7.43 5.84\n429. 9.5   0.34 7.43 5.81\n430. 14.5   0.34 7.44 5.79\n431. 14.1   0.34 7.44 5.76\n432. 16.1   0.33 7.44 5.77\n433. 14.3   0.32 7.45 5.78\n434. 11.7   0.32 7.45 5.75\n435. 13.4   0.32 7.45 5.72\n436. 9.6   0.32 7.45 5.73\n437. 8.7   0.31 7.46 5.70\n438. 8.4   0.31 7.46 5.67\n439. 12.8   0.30 7.46 5.64\n440. 10.5   0.30 7.47 5.62\n441. 17.1   0.30 7.47 5.59\n442. 18.4   0.30 7.47 5.56\n443. 15.4   0.30 7.47 5.53\n444. 10.8   0.30 7.47 5.50\n445. 11.8   0.29 7.48 5.47\n446. 14.9   0.29 7.48 5.44\n447. 12.6   0.28 7.49 5.45\n448. 14.1   0.27 7.49 5.42\n449. 13.0   0.27 7.50 5.42\n450. 13.4   0.26 7.50 5.39\n451. 15.2   0.26 7.50 5.35\n452. 16.1   0.25 7.50 5.36\n453. 17.8   0.25 7.50 5.32\n454. 14.9   0.25 7.50 5.29\n455. 14.1   0.25 7.50 5.29\n456. 12.7   0.25 7.50 5.25\n457. 13.5   0.25 7.50 5.22\n458. 14.9   0.24 7.50 5.18\n459. 20.0   0.24 7.50 5.14\n460. 16.4   0.24 7.50 5.10\n461. 17.7   0.24 7.51 5.06\n462. 19.5   0.23 7.51 5.02\n463. 20.2   0.23 7.51 4.97\n464. 21.4   0.23 7.51 4.93\n465. 19.9   0.22 7.51 4.93\n466. 19.0   0.22 7.51 4.93\n467. 19.1   0.21 7.52 4.89\n468. 19.1   0.21 7.52 4.91\n469. 20.1   0.20 7.52 4.92\n470. 19.9   0.19 7.52 4.93\n471. 19.6   0.19 7.52 4.88\n472. 23.2   0.19 7.51 4.83\n473. 29.8   0.19 7.51 4.78\n474. 13.8   0.19 7.51 4.73\n475. 13.3   0.19 7.51 4.67\n476. 16.7   0.19 7.51 4.62\n477. 12.0   0.18 7.51 4.56\n478. 14.6   0.17 7.52 4.49\n479. 21.4   0.17 7.52 4.43\n480. 23.0   0.17 7.52 4.36\n481. 23.7   0.17 7.52 4.29\n482. 25.0   0.16 7.52 4.22\n483. 21.8   0.16 7.52 4.14\n484. 20.6   0.15 7.52 4.15\n485. 21.2   0.14 7.52 4.20\n486. 19.1   0.14 7.52 4.22\n487. 20.6   0.13 7.52 4.14\n488. 15.2   0.13 7.52 4.17\n489. 7.0   0.12 7.52 4.09\n490. 8.1   0.12 7.52 4.00\n491. 13.6   0.11 7.53 3.91\n492. 20.1   0.10 7.53 3.81\n493. 21.8   0.10 7.53 3.70\n494. 24.5   0.09 7.53 3.58\n495. 23.1   0.09 7.53 3.46\n496. 19.7   0.08 7.53 3.32\n497. 18.3   0.08 7.53 3.17\n498. 21.2   0.07 7.53 3.00\n499. 17.5   0.06 7.53 2.81\n500. 16.8   0.06 7.53 2.58\n501. 22.4   0.05 7.53 2.32\n502. 20.6   0.04 7.53 2.00\n503. 23.9   0.03 7.53 1.58\n504. 22.0   0.03 7.53 1.00\n505. 11.9   0.02 7.53 0.00\nДеление после элемента:372\n","output_type":"stream"}]},{"cell_type":"code","source":"# посмотрим на информационный выигрыш\nIG(HEntropy, medv, 372)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.53518054886991"},"metadata":{}}]},{"cell_type":"code","source":"HGini(medv)","metadata":{"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"0.9931181552594163"},"metadata":{}}]},{"cell_type":"code","source":"# посмотрим на критерий Джини\ntest_H(HGini, medv)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"#     l     IG   Hl   Hr  \n------------------------\n 0. 24.0   0.00 0.00 0.99\n 1. 21.6   0.00 0.00 0.99\n 2. 34.7   0.00 0.50 0.99\n 3. 33.4   0.00 0.67 0.99\n 4. 36.2   0.00 0.75 0.99\n 5. 28.7   0.00 0.80 0.99\n 6. 22.9   0.00 0.83 0.99\n 7. 27.1   0.00 0.86 0.99\n 8. 16.5   0.00 0.88 0.99\n 9. 18.9   0.00 0.89 0.99\n10. 15.0   0.00 0.90 0.99\n11. 18.9   0.00 0.91 0.99\n12. 21.7   0.00 0.90 0.99\n13. 20.4   0.00 0.91 0.99\n14. 18.2   0.00 0.92 0.99\n15. 19.9   0.00 0.92 0.99\n16. 23.1   0.00 0.93 0.99\n17. 17.5   0.00 0.93 0.99\n18. 20.2   0.00 0.94 0.99\n19. 18.2   0.00 0.94 0.99\n20. 13.6   0.00 0.94 0.99\n21. 19.6   0.00 0.94 0.99\n22. 15.2   0.00 0.95 0.99\n23. 14.5   0.00 0.95 0.99\n24. 15.6   0.00 0.95 0.99\n25. 13.9   0.00 0.95 0.99\n26. 16.6   0.00 0.96 0.99\n27. 14.8   0.00 0.96 0.99\n28. 18.4   0.00 0.96 0.99\n29. 21.0   0.00 0.96 0.99\n30. 12.7   0.00 0.96 0.99\n31. 14.5   0.00 0.96 0.99\n32. 13.2   0.00 0.96 0.99\n33. 13.1   0.00 0.96 0.99\n34. 13.5   0.00 0.97 0.99\n35. 18.9   0.00 0.97 0.99\n36. 20.0   0.00 0.96 0.99\n37. 21.0   0.00 0.97 0.99\n38. 24.7   0.00 0.97 0.99\n39. 30.8   0.00 0.97 0.99\n40. 34.9   0.00 0.97 0.99\n41. 26.6   0.00 0.97 0.99\n42. 25.3   0.00 0.97 0.99\n43. 24.7   0.00 0.97 0.99\n44. 21.2   0.00 0.97 0.99\n45. 19.3   0.00 0.97 0.99\n46. 20.0   0.00 0.97 0.99\n47. 16.6   0.00 0.97 0.99\n48. 14.4   0.00 0.97 0.99\n49. 19.4   0.00 0.97 0.99\n50. 19.7   0.00 0.97 0.99\n51. 20.5   0.00 0.97 0.99\n52. 25.0   0.00 0.97 0.99\n53. 23.4   0.00 0.97 0.99\n54. 18.9   0.00 0.98 0.99\n55. 35.4   0.00 0.97 0.99\n56. 24.7   0.00 0.97 0.99\n57. 31.6   0.00 0.97 0.99\n58. 23.3   0.00 0.97 0.99\n59. 19.6   0.00 0.98 0.99\n60. 18.7   0.00 0.98 0.99\n61. 16.0   0.00 0.98 0.99\n62. 22.2   0.00 0.98 0.99\n63. 25.0   0.00 0.98 0.99\n64. 33.0   0.00 0.98 0.99\n65. 23.5   0.00 0.98 0.99\n66. 19.4   0.00 0.98 0.99\n67. 22.0   0.00 0.98 0.99\n68. 17.4   0.00 0.98 0.99\n69. 20.9   0.00 0.98 0.99\n70. 24.2   0.00 0.98 0.99\n71. 21.7   0.00 0.98 0.99\n72. 22.8   0.00 0.98 0.99\n73. 23.4   0.00 0.98 0.99\n74. 24.1   0.00 0.98 0.99\n75. 21.4   0.00 0.98 0.99\n76. 20.0   0.00 0.98 0.99\n77. 20.8   0.00 0.98 0.99\n78. 21.2   0.00 0.98 0.99\n79. 20.3   0.00 0.98 0.99\n80. 28.0   0.00 0.98 0.99\n81. 23.9   0.00 0.98 0.99\n82. 24.8   0.00 0.98 0.99\n83. 22.9   0.00 0.98 0.99\n84. 23.9   0.00 0.98 0.99\n85. 26.6   0.00 0.98 0.99\n86. 22.5   0.00 0.98 0.99\n87. 22.2   0.00 0.98 0.99\n88. 23.6   0.00 0.98 0.99\n89. 28.7   0.00 0.98 0.99\n90. 22.6   0.00 0.98 0.99\n91. 22.0   0.00 0.98 0.99\n92. 22.9   0.00 0.98 0.99\n93. 25.0   0.00 0.98 0.99\n94. 20.6   0.00 0.98 0.99\n95. 28.4   0.00 0.98 0.99\n96. 21.4   0.00 0.98 0.99\n97. 38.7   0.00 0.98 0.99\n98. 43.8   0.00 0.98 0.99\n99. 33.2   0.00 0.98 0.99\n100. 27.5   0.00 0.98 0.99\n101. 26.5   0.00 0.98 0.99\n102. 18.6   0.00 0.98 0.99\n103. 19.3   0.00 0.98 0.99\n104. 20.1   0.00 0.98 0.99\n105. 19.5   0.00 0.98 0.99\n106. 19.5   0.00 0.98 0.99\n107. 20.4   0.00 0.98 0.99\n108. 19.8   0.00 0.98 0.99\n109. 19.4   0.00 0.98 0.99\n110. 21.7   0.00 0.98 0.99\n111. 22.8   0.00 0.98 0.99\n112. 18.8   0.00 0.98 0.99\n113. 18.7   0.00 0.98 0.99\n114. 18.5   0.00 0.98 0.99\n115. 18.3   0.00 0.98 0.99\n116. 21.2   0.00 0.99 0.99\n117. 19.2   0.00 0.99 0.99\n118. 20.4   0.00 0.99 0.99\n119. 19.3   0.00 0.99 0.99\n120. 22.0   0.00 0.98 0.99\n121. 20.3   0.00 0.98 0.99\n122. 20.5   0.00 0.98 0.99\n123. 17.3   0.00 0.98 0.99\n124. 18.8   0.00 0.99 0.99\n125. 21.4   0.00 0.99 0.99\n126. 15.7   0.00 0.99 0.99\n127. 16.2   0.00 0.99 0.99\n128. 18.0   0.00 0.99 0.99\n129. 14.3   0.00 0.99 0.99\n130. 19.2   0.00 0.99 0.99\n131. 19.6   0.00 0.99 0.99\n132. 23.0   0.00 0.99 0.99\n133. 18.4   0.00 0.99 0.99\n134. 15.6   0.00 0.99 0.99\n135. 18.1   0.00 0.99 0.99\n136. 17.4   0.00 0.99 0.99\n137. 17.1   0.00 0.99 0.99\n138. 13.3   0.00 0.99 0.99\n139. 17.8   0.00 0.99 0.99\n140. 14.0   0.00 0.99 0.99\n141. 14.4   0.00 0.99 0.99\n142. 13.4   0.00 0.99 0.99\n143. 15.6   0.00 0.99 0.99\n144. 11.8   0.00 0.99 0.99\n145. 13.8   0.00 0.99 0.99\n146. 15.6   0.00 0.99 0.99\n147. 14.6   0.00 0.99 0.99\n148. 17.8   0.00 0.99 0.99\n149. 15.4   0.00 0.99 0.99\n150. 21.5   0.00 0.99 0.99\n151. 19.6   0.00 0.99 0.99\n152. 15.3   0.00 0.99 0.99\n153. 19.4   0.00 0.99 0.99\n154. 17.0   0.00 0.99 0.99\n155. 15.6   0.00 0.99 0.99\n156. 13.1   0.00 0.99 0.99\n157. 41.3   0.00 0.99 0.99\n158. 24.3   0.00 0.99 0.99\n159. 23.3   0.00 0.99 0.99\n160. 27.0   0.00 0.99 0.99\n161. 50.0   0.00 0.99 0.99\n162. 50.0   0.00 0.99 0.99\n163. 50.0   0.00 0.99 0.99\n164. 22.7   0.00 0.99 0.99\n165. 25.0   0.00 0.99 0.99\n166. 50.0   0.00 0.99 0.99\n167. 23.8   0.00 0.99 0.99\n168. 23.8   0.00 0.99 0.99\n169. 22.3   0.00 0.99 0.99\n170. 17.4   0.00 0.99 0.99\n171. 19.1   0.00 0.99 0.99\n172. 23.1   0.00 0.99 0.99\n173. 23.6   0.00 0.99 0.99\n174. 22.6   0.00 0.99 0.99\n175. 29.4   0.00 0.99 0.99\n176. 23.2   0.00 0.99 0.99\n177. 24.6   0.00 0.99 0.99\n178. 29.9   0.00 0.99 0.99\n179. 37.2   0.00 0.99 0.99\n180. 39.8   0.00 0.99 0.99\n181. 36.2   0.00 0.99 0.99\n182. 37.9   0.00 0.99 0.99\n183. 32.5   0.00 0.99 0.99\n184. 26.4   0.00 0.99 0.99\n185. 29.6   0.00 0.99 0.99\n186. 50.0   0.00 0.99 0.99\n187. 32.0   0.00 0.99 0.99\n188. 29.8   0.00 0.99 0.99\n189. 34.9   0.00 0.99 0.99\n190. 37.0   0.00 0.99 0.99\n191. 30.5   0.00 0.99 0.99\n192. 36.4   0.00 0.99 0.99\n193. 31.1   0.00 0.99 0.99\n194. 29.1   0.00 0.99 0.99\n195. 50.0   0.00 0.99 0.99\n196. 33.3   0.00 0.99 0.99\n197. 30.3   0.00 0.99 0.99\n198. 34.6   0.00 0.99 0.99\n199. 34.9   0.00 0.99 0.99\n200. 32.9   0.00 0.99 0.99\n201. 24.1   0.00 0.99 0.99\n202. 42.3   0.00 0.99 0.99\n203. 48.5   0.00 0.99 0.99\n204. 50.0   0.00 0.99 0.99\n205. 22.6   0.00 0.99 0.99\n206. 24.4   0.00 0.99 0.99\n207. 22.5   0.00 0.99 0.99\n208. 24.4   0.00 0.99 0.99\n209. 20.0   0.00 0.99 0.99\n210. 21.7   0.00 0.99 0.99\n211. 19.3   0.00 0.99 0.99\n212. 22.4   0.00 0.99 0.99\n213. 28.1   0.00 0.99 0.99\n214. 23.7   0.00 0.99 0.99\n215. 25.0   0.00 0.99 0.99\n216. 23.3   0.00 0.99 0.99\n217. 28.7   0.00 0.99 0.99\n218. 21.5   0.00 0.99 0.99\n219. 23.0   0.00 0.99 0.99\n220. 26.7   0.00 0.99 0.99\n221. 21.7   0.00 0.99 0.99\n222. 27.5   0.00 0.99 0.99\n223. 30.1   0.00 0.99 0.99\n224. 44.8   0.00 0.99 0.99\n225. 50.0   0.00 0.99 0.99\n226. 37.6   0.00 0.99 0.99\n227. 31.6   0.00 0.99 0.99\n228. 46.7   0.00 0.99 0.99\n229. 31.5   0.00 0.99 0.99\n230. 24.3   0.00 0.99 0.99\n231. 31.7   0.00 0.99 0.99\n232. 41.7   0.00 0.99 0.99\n233. 48.3   0.00 0.99 0.99\n234. 29.0   0.00 0.99 0.99\n235. 24.0   0.00 0.99 0.99\n236. 25.1   0.00 0.99 0.99\n237. 31.5   0.00 0.99 0.99\n238. 23.7   0.00 0.99 0.99\n239. 23.3   0.00 0.99 0.99\n240. 22.0   0.00 0.99 0.99\n241. 20.1   0.00 0.99 0.99\n242. 22.2   0.00 0.99 0.99\n243. 23.7   0.00 0.99 0.99\n244. 17.6   0.00 0.99 0.99\n245. 18.5   0.00 0.99 0.99\n246. 24.3   0.00 0.99 0.99\n247. 20.5   0.00 0.99 0.99\n248. 24.5   0.00 0.99 0.99\n249. 26.2   0.00 0.99 0.99\n250. 24.4   0.00 0.99 0.99\n251. 24.8   0.00 0.99 0.99\n252. 29.6   0.00 0.99 0.99\n253. 42.8   0.00 0.99 0.99\n254. 21.9   0.00 0.99 0.99\n255. 20.9   0.00 0.99 0.99\n256. 44.0   0.00 0.99 0.99\n257. 50.0   0.00 0.99 0.99\n258. 36.0   0.00 0.99 0.99\n259. 30.1   0.00 0.99 0.99\n260. 33.8   0.00 0.99 0.99\n261. 43.1   0.00 0.99 0.99\n262. 48.8   0.00 0.99 0.99\n263. 31.0   0.00 0.99 0.99\n264. 36.5   0.00 0.99 0.99\n265. 22.8   0.00 0.99 0.99\n266. 30.7   0.00 0.99 0.99\n267. 50.0   0.00 0.99 0.99\n268. 43.5   0.00 0.99 0.99\n269. 20.7   0.00 0.99 0.99\n270. 21.1   0.00 0.99 0.99\n271. 25.2   0.00 0.99 0.99\n272. 24.4   0.00 0.99 0.99\n273. 35.2   0.00 0.99 0.99\n274. 32.4   0.00 0.99 0.99\n275. 32.0   0.00 0.99 0.99\n276. 33.2   0.00 0.99 0.99\n277. 33.1   0.00 0.99 0.99\n278. 29.1   0.00 0.99 0.99\n279. 35.1   0.00 0.99 0.99\n280. 45.4   0.00 0.99 0.99\n281. 35.4   0.00 0.99 0.99\n282. 46.0   0.00 0.99 0.99\n283. 50.0   0.00 0.99 0.99\n284. 32.2   0.00 0.99 0.99\n285. 22.0   0.00 0.99 0.99\n286. 20.1   0.00 0.99 0.99\n287. 23.2   0.00 0.99 0.99\n288. 22.3   0.00 0.99 0.99\n289. 24.8   0.00 0.99 0.99\n290. 28.5   0.00 0.99 0.99\n291. 37.3   0.00 0.99 0.99\n292. 27.9   0.00 0.99 0.99\n293. 23.9   0.00 0.99 0.99\n294. 21.7   0.00 0.99 0.99\n295. 28.6   0.00 0.99 0.99\n296. 27.1   0.00 0.99 0.99\n297. 20.3   0.00 0.99 0.99\n298. 22.5   0.00 0.99 0.99\n299. 29.0   0.00 0.99 0.99\n300. 24.8   0.00 0.99 0.99\n301. 22.0   0.00 0.99 0.99\n302. 26.4   0.00 0.99 0.99\n303. 33.1   0.00 0.99 0.99\n304. 36.1   0.00 0.99 0.99\n305. 28.4   0.00 0.99 0.99\n306. 33.4   0.00 0.99 0.99\n307. 28.2   0.00 0.99 0.99\n308. 22.8   0.00 0.99 0.99\n309. 20.3   0.00 0.99 0.99\n310. 16.1   0.00 0.99 0.99\n311. 22.1   0.00 0.99 0.99\n312. 19.4   0.00 0.99 0.99\n313. 21.6   0.00 0.99 0.99\n314. 23.8   0.00 0.99 0.99\n315. 16.2   0.00 0.99 0.99\n316. 17.8   0.00 0.99 0.99\n317. 19.8   0.00 0.99 0.99\n318. 23.1   0.00 0.99 0.99\n319. 21.0   0.00 0.99 0.99\n320. 23.8   0.00 0.99 0.99\n321. 23.1   0.00 0.99 0.99\n322. 20.4   0.00 0.99 0.99\n323. 18.5   0.00 0.99 0.99\n324. 25.0   0.00 0.99 0.99\n325. 24.6   0.00 0.99 0.99\n326. 23.0   0.00 0.99 0.99\n327. 22.2   0.00 0.99 0.99\n328. 19.3   0.00 0.99 0.99\n329. 22.6   0.00 0.99 0.99\n330. 19.8   0.00 0.99 0.99\n331. 17.1   0.00 0.99 0.99\n332. 19.4   0.00 0.99 0.99\n333. 22.2   0.00 0.99 0.99\n334. 20.7   0.00 0.99 0.99\n335. 21.1   0.00 0.99 0.99\n336. 19.5   0.00 0.99 0.99\n337. 18.5   0.00 0.99 0.99\n338. 20.6   0.00 0.99 0.99\n339. 19.0   0.00 0.99 0.99\n340. 18.7   0.00 0.99 0.99\n341. 32.7   0.00 0.99 0.99\n342. 16.5   0.00 0.99 0.99\n343. 23.9   0.00 0.99 0.99\n344. 31.2   0.00 0.99 0.99\n345. 17.5   0.00 0.99 0.99\n346. 17.2   0.00 0.99 0.99\n347. 23.1   0.00 0.99 0.99\n348. 24.5   0.00 0.99 0.99\n349. 26.6   0.00 0.99 0.99\n350. 22.9   0.00 0.99 0.99\n351. 24.1   0.00 0.99 0.99\n352. 18.6   0.00 0.99 0.99\n353. 30.1   0.00 0.99 0.99\n354. 18.2   0.00 0.99 0.99\n355. 20.6   0.00 0.99 0.99\n356. 17.8   0.00 0.99 0.99\n357. 21.7   0.00 0.99 0.99\n358. 22.7   0.00 0.99 0.99\n359. 22.6   0.00 0.99 0.99\n360. 25.0   0.00 0.99 0.99\n361. 19.9   0.00 0.99 0.99\n362. 20.8   0.00 0.99 0.99\n363. 16.8   0.00 0.99 0.99\n364. 21.9   0.00 0.99 0.99\n365. 27.5   0.00 0.99 0.99\n366. 21.9   0.00 0.99 0.99\n367. 23.1   0.00 0.99 0.99\n368. 50.0   0.00 0.99 0.99\n369. 50.0   0.00 0.99 0.99\n370. 50.0   0.00 0.99 0.99\n371. 50.0   0.00 0.99 0.99\n372. 50.0   0.00 0.99 0.99\n373. 13.8   0.00 0.99 0.99\n374. 13.8   0.00 0.99 0.99\n375. 15.0   0.00 0.99 0.99\n376. 13.9   0.00 0.99 0.99\n377. 13.3   0.00 0.99 0.99\n378. 13.1   0.00 0.99 0.99\n379. 10.2   0.00 0.99 0.99\n380. 10.4   0.00 0.99 0.99\n381. 10.9   0.00 0.99 0.99\n382. 11.3   0.00 0.99 0.99\n383. 12.3   0.00 0.99 0.99\n384. 8.8   0.00 0.99 0.99\n385. 7.2   0.00 0.99 0.99\n386. 10.5   0.00 0.99 0.99\n387. 7.4   0.00 0.99 0.99\n388. 10.2   0.00 0.99 0.99\n389. 11.5   0.00 0.99 0.99\n390. 15.1   0.00 0.99 0.99\n391. 23.2   0.00 0.99 0.99\n392. 9.7   0.00 0.99 0.99\n393. 13.8   0.00 0.99 0.99\n394. 12.7   0.00 0.99 0.99\n395. 13.1   0.00 0.99 0.99\n396. 12.5   0.00 0.99 0.99\n397. 8.5   0.00 0.99 0.99\n398. 5.0   0.00 0.99 0.99\n399. 6.3   0.00 0.99 0.99\n400. 5.6   0.00 0.99 0.99\n401. 7.2   0.00 0.99 0.99\n402. 12.1   0.00 0.99 0.99\n403. 8.3   0.00 0.99 0.98\n404. 8.5   0.00 0.99 0.98\n405. 5.0   0.00 0.99 0.98\n406. 11.9   0.00 0.99 0.98\n407. 27.9   0.00 0.99 0.98\n408. 17.2   0.00 0.99 0.98\n409. 27.5   0.00 0.99 0.98\n410. 15.0   0.00 0.99 0.98\n411. 17.2   0.00 0.99 0.98\n412. 17.9   0.00 0.99 0.98\n413. 16.3   0.00 0.99 0.98\n414. 7.0   0.00 0.99 0.98\n415. 7.2   0.00 0.99 0.98\n416. 7.5   0.00 0.99 0.98\n417. 10.4   0.00 0.99 0.98\n418. 8.8   0.00 0.99 0.98\n419. 8.4   0.00 0.99 0.98\n420. 16.7   0.00 0.99 0.98\n421. 14.2   0.00 0.99 0.98\n422. 20.8   0.00 0.99 0.98\n423. 13.4   0.00 0.99 0.98\n424. 11.7   0.00 0.99 0.98\n425. 8.3   0.00 0.99 0.98\n426. 10.2   0.00 0.99 0.98\n427. 10.9   0.00 0.99 0.98\n428. 11.0   0.00 0.99 0.98\n429. 9.5   0.00 0.99 0.98\n430. 14.5   0.00 0.99 0.98\n431. 14.1   0.00 0.99 0.98\n432. 16.1   0.00 0.99 0.98\n433. 14.3   0.00 0.99 0.98\n434. 11.7   0.00 0.99 0.98\n435. 13.4   0.00 0.99 0.98\n436. 9.6   0.00 0.99 0.98\n437. 8.7   0.00 0.99 0.98\n438. 8.4   0.00 0.99 0.98\n439. 12.8   0.00 0.99 0.98\n440. 10.5   0.00 0.99 0.98\n441. 17.1   0.00 0.99 0.98\n442. 18.4   0.00 0.99 0.98\n443. 15.4   0.00 0.99 0.98\n444. 10.8   0.00 0.99 0.98\n445. 11.8   0.00 0.99 0.98\n446. 14.9   0.00 0.99 0.97\n447. 12.6   0.00 0.99 0.98\n448. 14.1   0.00 0.99 0.97\n449. 13.0   0.00 0.99 0.97\n450. 13.4   0.00 0.99 0.97\n451. 15.2   0.00 0.99 0.97\n452. 16.1   0.00 0.99 0.97\n453. 17.8   0.00 0.99 0.97\n454. 14.9   0.00 0.99 0.97\n455. 14.1   0.00 0.99 0.97\n456. 12.7   0.00 0.99 0.97\n457. 13.5   0.00 0.99 0.97\n458. 14.9   0.00 0.99 0.97\n459. 20.0   0.00 0.99 0.97\n460. 16.4   0.00 0.99 0.97\n461. 17.7   0.00 0.99 0.97\n462. 19.5   0.00 0.99 0.97\n463. 20.2   0.00 0.99 0.96\n464. 21.4   0.00 0.99 0.96\n465. 19.9   0.00 0.99 0.96\n466. 19.0   0.00 0.99 0.96\n467. 19.1   0.00 0.99 0.96\n468. 19.1   0.00 0.99 0.96\n469. 20.1   0.00 0.99 0.96\n470. 19.9   0.00 0.99 0.96\n471. 19.6   0.00 0.99 0.96\n472. 23.2   0.00 0.99 0.96\n473. 29.8   0.00 0.99 0.96\n474. 13.8   0.00 0.99 0.96\n475. 13.3   0.00 0.99 0.96\n476. 16.7   0.00 0.99 0.96\n477. 12.0   0.00 0.99 0.95\n478. 14.6   0.00 0.99 0.95\n479. 21.4   0.00 0.99 0.95\n480. 23.0   0.00 0.99 0.95\n481. 23.7   0.00 0.99 0.94\n482. 25.0   0.00 0.99 0.94\n483. 21.8   0.00 0.99 0.94\n484. 20.6   0.00 0.99 0.94\n485. 21.2   0.00 0.99 0.94\n486. 19.1   0.00 0.99 0.94\n487. 20.6   0.00 0.99 0.94\n488. 15.2   0.00 0.99 0.94\n489. 7.0   0.00 0.99 0.94\n490. 8.1   0.00 0.99 0.94\n491. 13.6   0.00 0.99 0.93\n492. 20.1   0.00 0.99 0.93\n493. 21.8   0.00 0.99 0.92\n494. 24.5   0.00 0.99 0.92\n495. 23.1   0.00 0.99 0.91\n496. 19.7   0.00 0.99 0.90\n497. 18.3   0.00 0.99 0.89\n498. 21.2   0.00 0.99 0.88\n499. 17.5   0.00 0.99 0.86\n500. 16.8   0.00 0.99 0.83\n501. 22.4   0.00 0.99 0.80\n502. 20.6   0.00 0.99 0.75\n503. 23.9   0.00 0.99 0.67\n504. 22.0   0.00 0.99 0.50\n505. 11.9   0.00 0.99 0.00\nДеление после элемента:372\n","output_type":"stream"}]},{"cell_type":"code","source":"# посмотрим на информационный выигрыш\nIG(HGini, medv, 372)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"0.0031308864523600377"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}